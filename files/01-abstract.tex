%!TEX root=/home/ska124/Dropbox/Thesis/thes-full.tex
%% Copyright 1998 Pepe Kubon
%%
%% `abstract.tex' --- abstract for thes-full.tex, thes-short-tex from
%%                    the `csthesis' bundle
%%
%% You are allowed to distribute this file together with all files
%% mentioned in READ.ME.
%%
%% You are not allowed to modify its contents.
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       Abstract 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\prefacesection{Abstract}

Memory in modern computing systems are hierarchical in
nature. Maintaining a memory hierarchy enables the system to service
frequently requested data from a small low latency store located close
to the processor. The design paradigms of the memory hierarchy have
been mostly unchanged since their inception in the late
1960's. However in the meantime there have been significant changes in
the tasks computers perform and the way they are programmed. Modern
computing systems perform more data centric tasks and are programmed
in higher level languages which introduce many layers of abstraction
between the programmer and the system. \\ 

Waste in the memory hierarchy refers to the under utilized space in
the memory system and consequently wasted energy and time. The data
access patterns of modern workloads are increasingly less uniform
which makes it hard to design a memory hierarchy with rigid design
principles that performs optimally for a wide range of workloads.  The
problem is exacerbated by the implications of the growing fraction of
dark silicon on a processor chip. \\

This dissertation proposes and evaluates the benefits of a novel
architecture for the on chip memory hierarchy which would allow it to
dynamically adapt to the requirements of the application. We propose a
design that can support a variable number of cache blocks, each of a
different granularity. It employs a novel organization that completely
eliminates the tag array, treating the storage array as uniform and
morph-able between tags and data.  This enables the cache to harvest
space from unused words in blocks for additional tag storage, thereby
supporting a variable number of tags (and correspondingly, blocks).
The design adjusts individual cache line granularity according to
the spatial locality in the application.  It adapts to the appropriate
granularity both for different data objects in an application as well
as for different phases of access to the same data. \\

Compared to a fixed granularity cache, improves cache utilization to
90\% - 99\% for most applications, saves miss rate by up to 73\% at
the L1 level and up to 88\% at the LLC level, and reduces miss
bandwidth by up to 84\% at the L1 and 92\% at the LLC. Correspondingly
reduces on-chip memory hierarchy energy by as much as 36\% and
improves performance by as much as 50\%.
